{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "Take reference seq and use translate tool to get sequence.\n",
    "\n",
    "https://www.mavedb.org/#/score-sets/urn:mavedb:00000086-a-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_type_seq = \"MSIQHFRVAL IPFFAAFCLP VFAHPETLVK VKDAEDQLGA RVGYIELDLN SGKILESFRP EERFPMMSTF KVLLCGAVLS RVDAGQEQLG RRIHYSQNDL VEYSPVTEKH LTDGMTVREL CSAAITMSDN TAANLLLTTI GGPKELTAFL HNMGDHVTRL DRWEPELNEA IPNDERDTTM PAAMATTLRK LLTGELLTLA SRQQLIDWME ADKVAGPLLR SALPAGWFIA DKSGAGERGS RGIIAALGPD GKPSRIVVIY TTGSQATMDE RNRQIAEIGA SLIKHW\".replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio.SeqUtils as bio\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "csv = pd.read_csv(\"E6.csv\", usecols=[\"hgvs_pro\",\"score\"])\n",
    "X_seqs = []\n",
    "y = []\n",
    "for index, row in csv.iterrows():\n",
    "    end = row[\"hgvs_pro\"][-3:]\n",
    "    position = int(re.findall(r'\\d+', row[\"hgvs_pro\"])[0])-1\n",
    "    mutant_seq = wild_type_seq\n",
    "    mutant_seq = list(mutant_seq)\n",
    "    mutant_seq[position] = bio.IUPACData.protein_letters_3to1[end]\n",
    "    if not pd.isna(row[\"score\"]):\n",
    "        X_seqs.append(mutant_seq)\n",
    "        y.append(float(row[\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "enc = OneHotEncoder()\n",
    "X_onehot = enc.fit_transform(X_seqs).toarray()\n",
    "X_tensor = torch.tensor(X_onehot, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/anthosbz/Documents/Learning_ML/First_ML/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | fc1     | Linear           | 329 K \n",
      "1 | fc2     | Linear           | 4.2 K \n",
      "2 | fc3     | Linear           | 65    \n",
      "3 | relu    | ReLU             | 0     \n",
      "4 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "333 K     Trainable params\n",
      "0         Non-trainable params\n",
      "333 K     Total params\n",
      "1.336     Total estimated model params size (MB)\n",
      "/home/anthosbz/Documents/Learning_ML/First_ML/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/61 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 61/61 [00:00<00:00, 170.10it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 61/61 [00:00<00:00, 166.65it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming you have X_train, y_train, X_test, and y_test tensors\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "input_size = X_train.size(1)\n",
    "output_size = torch.unique(y_train).size(0)\n",
    "hidden_size = 64  # Adjust as needed\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
